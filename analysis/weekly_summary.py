import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv
import os

# âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'Meiryo'

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
csv_dir = Path("data")
output_dir = Path("output")
output_dir.mkdir(exist_ok=True)

# CSVã®çµ±åˆ
all_files = list(csv_dir.glob("tweets_*_labeled.csv"))
if not all_files:
    print("âš ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚collector.py â†’ classifier.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    exit()

df_list = [pd.read_csv(f) for f in all_files]
df = pd.concat(df_list, ignore_index=True)

# æ—¥æ™‚å¤‰æ›ã¨æ™‚é–“å¸¯åˆ†é¡
df["created_at"] = pd.to_datetime(df["created_at"])
df["hour"] = df["created_at"].dt.hour

def classify_time(hour):
    if 5 <= hour < 11:
        return "æœ"
    elif 11 <= hour < 17:
        return "æ˜¼"
    elif 17 <= hour < 22:
        return "å¤•æ–¹"
    else:
        return "å¤œ"

df["æ™‚é–“å¸¯"] = df["hour"].apply(classify_time)

# ã‚«ãƒ†ã‚´ãƒªÃ—æ™‚é–“å¸¯ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆ
pivot = pd.pivot_table(df, index="category", columns="æ™‚é–“å¸¯", values="text", aggfunc="count", fill_value=0)

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»
plt.figure(figsize=(10, 6))
sns.heatmap(pivot, annot=True, fmt="d", cmap="YlGnBu")
plt.title("ã‚«ãƒ†ã‚´ãƒªÃ—æ™‚é–“å¸¯ å‡ºç¾æ•°ï¼ˆé€±æ¬¡ï¼‰", fontsize=14)
plt.xlabel("æ™‚é–“å¸¯")
plt.ylabel("ã‚«ãƒ†ã‚´ãƒª")
plt.tight_layout()
heatmap_path = output_dir / "heatmap_weekly.png"
plt.savefig(heatmap_path)
plt.close()

# æ™‚é–“å¸¯ã”ã¨ã®å¹³å‡ã„ã„ã­æ•°ï¼ˆbar plotï¼‰
likes_mean = df.groupby("æ™‚é–“å¸¯")["likes"].mean().reindex(["æœ", "æ˜¼", "å¤•æ–¹", "å¤œ"])
plt.figure(figsize=(6, 4))
likes_mean.plot(kind="bar", color="skyblue")
plt.title("æ™‚é–“å¸¯åˆ¥ã®å¹³å‡ã„ã„ã­æ•°")
plt.ylabel("å¹³å‡ã„ã„ã­æ•°")
plt.xlabel("æ™‚é–“å¸¯")
plt.tight_layout()
likes_plot_path = output_dir / "likes_by_time.png"
plt.savefig(likes_plot_path)
plt.close()

# HTMLãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦ä¿å­˜ï¼ˆç°¡æ˜“ï¼‰
today_str = datetime.now().strftime("%Y%m%d")
report_path = output_dir / f"weekly_report_{today_str}.html"
with open(report_path, "w", encoding="utf-8") as f:
    f.write(f"""
    <html>
    <head><meta charset="utf-8"><title>é€±æ¬¡Twitteråˆ†æãƒ¬ãƒãƒ¼ãƒˆ</title></head>
    <body>
    <h1>é€±æ¬¡Twitteråˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆ{today_str}ï¼‰</h1>
    <h2>1. ã‚«ãƒ†ã‚´ãƒªÃ—æ™‚é–“å¸¯ å‡ºç¾ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—</h2>
    <img src="{heatmap_path.name}" width="600"><br>
    <h2>2. æ™‚é–“å¸¯åˆ¥ å¹³å‡ã„ã„ã­æ•°</h2>
    <img src="{likes_plot_path.name}" width="400"><br>
    <p>å¯¾è±¡ãƒ„ã‚¤ãƒ¼ãƒˆæ•°ï¼š{len(df)} ä»¶</p>
    </body>
    </html>
    """)

print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å®Œäº† â†’ {report_path}")


# æŠ•ç¨¿æ–‡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
def generate_summary_text(df, pivot, likes_mean, date_str):
    prompt = f"""
ã‚ãªãŸã¯Twitteråˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æ¡ä»¶ã§é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆã®æ–‡ç« ã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# æ¡ä»¶:
- æ—¥ä»˜: {date_str}
- åˆ†æå¯¾è±¡ã®ã‚«ãƒ†ã‚´ãƒªã¨ãã®æŠ•ç¨¿æ•°: {dict(df['category'].value_counts())}
- å„æ™‚é–“å¸¯ã”ã¨ã®æŠ•ç¨¿ä»¶æ•°ã¨å¹³å‡ã„ã„ã­æ•°: {likes_mean.to_dict()}
- ã‚«ãƒ†ã‚´ãƒªÃ—æ™‚é–“å¸¯ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆçµæœ: {pivot.to_dict()}

# æŒ‡ç¤º:
- noteç”¨ã®èª­ã¿ã‚„ã™ã„é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆã®æœ¬æ–‡ã¨ã—ã¦
- å£èª¿ã¯ä¸å¯§ã§ã‚ã‹ã‚Šã‚„ã™ã
- ç®‡æ¡æ›¸ãã‚„è¦‹å‡ºã—ã‚’äº¤ãˆã¦
- SNSé‹ç”¨è€…ãŒå‚è€ƒã«ã§ãã‚‹å…·ä½“çš„ãªåˆ†æã‚’å«ã‚ã¦ãã ã•ã„
- 800æ–‡å­—ä»¥å†…ã§å‡ºåŠ›ã—ã¦ãã ã•ã„
    """
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯SNSãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®åˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=800
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âš ï¸ OpenAI API ã‚¨ãƒ©ãƒ¼: {e}"

# è¦ç´„æ–‡ã®ç”Ÿæˆ
summary_text = generate_summary_text(df, pivot, likes_mean, today_str)
summary_path = output_dir / f"weekly_summary_{today_str}.txt"
with open(summary_path, "w", encoding="utf-8") as f:
    f.write(summary_text)

print(f"ğŸ“ æŠ•ç¨¿æ–‡ç”Ÿæˆ â†’ {summary_path}")

import tweepy

def post_to_twitter(text_path):
    with open(text_path, encoding="utf-8") as f:
        content = f.read()

    # èª­ã¿è¾¼ã‚“ã è¦ç´„æ–‡ã‚’ãƒˆãƒªãƒ ã—ã¦280æ–‡å­—ä»¥å†…ã«åã‚ã‚‹ï¼ˆnoteèª˜å°ã‚‚å¯èƒ½ï¼‰
    max_length = 260  # noteãƒªãƒ³ã‚¯ä»˜ãã«ã—ãŸã„å ´åˆã¯260ã«ã™ã‚‹
    tweet_text = content[:max_length] + " #é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆ"

    # Twitterèªè¨¼
    auth = tweepy.OAuth1UserHandler(
        os.getenv("API_KEY"),
        os.getenv("API_SECRET"),
        os.getenv("ACCESS_TOKEN"),
        os.getenv("ACCESS_SECRET"),
    )
    api = tweepy.API(auth)

    try:
        api.update_status(status=tweet_text)
        print("âœ… Xï¼ˆTwitterï¼‰ã¸è‡ªå‹•æŠ•ç¨¿å®Œäº†ï¼")
    except Exception as e:
        print(f"âš ï¸ TwitteræŠ•ç¨¿ã‚¨ãƒ©ãƒ¼: {e}")

# --- Zennè¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ ---
zenn_path = Path("../zenn-content/articles")  # GitHubã«é€£æºã—ã¦ã„ã‚‹Zennç”¨ãƒªãƒã‚¸ãƒˆãƒª
zenn_path.mkdir(parents=True, exist_ok=True)

md_filename = f"weekly-report-{today_str}.md"
md_path = zenn_path / md_filename

with open(md_path, "w", encoding="utf-8") as f:
    f.write(f"""---
title: "Twitteré€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆ{today_str}ï¼‰"
emoji: "ğŸ“ˆ"
type: "idea"
topics: ["Twitter", "SNSåˆ†æ"]
published: true
---

{summary_text}
""")

print(f"ğŸ“ ZennæŠ•ç¨¿ç”¨Markdownç”Ÿæˆ â†’ {md_path}")
